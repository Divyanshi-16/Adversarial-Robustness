{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divyanshi-16/Adversarial-Robustness/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UYHSPVGpRH1"
      },
      "source": [
        "**Dataset:** MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model:** Convolutional Neural Network"
      ],
      "metadata": {
        "id": "yBZp4vg5G7Cw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGbSFUQJX5Kx",
        "outputId": "dc0733dc-96e1-4df6-f101-43902fb9cb9d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 96s 48ms/step - loss: 0.2447 - accuracy: 0.9388 - val_loss: 0.0962 - val_accuracy: 0.9841 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.1322 - accuracy: 0.9730 - val_loss: 0.1420 - val_accuracy: 0.9691 - lr: 9.0000e-04\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 85s 45ms/step - loss: 0.1178 - accuracy: 0.9786 - val_loss: 0.1020 - val_accuracy: 0.9827 - lr: 8.1000e-04\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 84s 45ms/step - loss: 0.1085 - accuracy: 0.9811 - val_loss: 0.0733 - val_accuracy: 0.9920 - lr: 7.2900e-04\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.0981 - accuracy: 0.9829 - val_loss: 0.0688 - val_accuracy: 0.9917 - lr: 6.5610e-04\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 85s 45ms/step - loss: 0.0928 - accuracy: 0.9851 - val_loss: 0.0700 - val_accuracy: 0.9917 - lr: 5.9049e-04\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.0849 - accuracy: 0.9858 - val_loss: 0.0629 - val_accuracy: 0.9922 - lr: 5.3144e-04\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 82s 44ms/step - loss: 0.0781 - accuracy: 0.9871 - val_loss: 0.0573 - val_accuracy: 0.9935 - lr: 4.7830e-04\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 85s 45ms/step - loss: 0.0726 - accuracy: 0.9884 - val_loss: 0.0600 - val_accuracy: 0.9920 - lr: 4.3047e-04\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0693 - accuracy: 0.9884 - val_loss: 0.0567 - val_accuracy: 0.9921 - lr: 3.8742e-04\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.0660 - accuracy: 0.9893 - val_loss: 0.0514 - val_accuracy: 0.9940 - lr: 3.4868e-04\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 84s 45ms/step - loss: 0.0623 - accuracy: 0.9899 - val_loss: 0.0514 - val_accuracy: 0.9926 - lr: 3.1381e-04\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 84s 45ms/step - loss: 0.0577 - accuracy: 0.9906 - val_loss: 0.0472 - val_accuracy: 0.9939 - lr: 2.8243e-04\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 82s 44ms/step - loss: 0.0558 - accuracy: 0.9914 - val_loss: 0.0480 - val_accuracy: 0.9930 - lr: 2.5419e-04\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 85s 45ms/step - loss: 0.0517 - accuracy: 0.9918 - val_loss: 0.0436 - val_accuracy: 0.9942 - lr: 2.2877e-04\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 87s 46ms/step - loss: 0.0500 - accuracy: 0.9915 - val_loss: 0.0435 - val_accuracy: 0.9937 - lr: 2.0589e-04\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 84s 45ms/step - loss: 0.0474 - accuracy: 0.9926 - val_loss: 0.0427 - val_accuracy: 0.9934 - lr: 1.8530e-04\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 87s 46ms/step - loss: 0.0455 - accuracy: 0.9930 - val_loss: 0.0404 - val_accuracy: 0.9938 - lr: 1.6677e-04\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 87s 46ms/step - loss: 0.0434 - accuracy: 0.9930 - val_loss: 0.0385 - val_accuracy: 0.9952 - lr: 1.5009e-04\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 86s 46ms/step - loss: 0.0414 - accuracy: 0.9935 - val_loss: 0.0372 - val_accuracy: 0.9952 - lr: 1.3509e-04\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0372 - accuracy: 0.9952\n",
            "Test accuracy: 0.995199978351593\n",
            "313/313 [==============================] - 4s 14ms/step\n",
            "Precision: 0.9952510408343596\n",
            "Recall: 0.9951096929467462\n"
          ]
        }
      ],
      "source": [
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "with ZipFile('mnist_train.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('mnist_train')\n",
        "\n",
        "with ZipFile('mnist_test.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('mnist_test')\n",
        "\n",
        "train_data = pd.read_csv('mnist_train/mnist_train.csv')\n",
        "test_data = pd.read_csv('mnist_test/mnist_test.csv')\n",
        "\n",
        "x_train = train_data.iloc[:, 1:].values\n",
        "y_train = train_data.iloc[:, 0].values\n",
        "x_test = test_data.iloc[:, 1:].values\n",
        "y_test = test_data.iloc[:, 0].values\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Reshape input data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), strides=(1, 1), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(64, kernel_size=(3, 3), strides=(1, 1), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    BatchNormalization(),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n",
        "    BatchNormalization(),\n",
        "    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n",
        "    BatchNormalization(),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "initial_learning_rate = 0.001\n",
        "opt = Adam(lr=initial_learning_rate)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Learning rate scheduling\n",
        "def lr_schedule(epoch):\n",
        "    return initial_learning_rate * 0.9 ** epoch\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    epochs=20,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[lr_scheduler])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_accuracy}')\n",
        "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initial Robustness:** FGSM White Box Attack"
      ],
      "metadata": {
        "id": "YErq8XfH7JM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_tf = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
        "\n",
        "# Generate adversarial examples using FGSM attack in TensorFlow\n",
        "eps = 0.3\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(x_test_tf)\n",
        "    predictions = model(x_test_tf)\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_test, predictions)\n",
        "gradients = tape.gradient(loss, x_test_tf)\n",
        "x_test_adv = tf.clip_by_value(x_test_tf + eps * tf.sign(gradients), 0, 1)\n",
        "\n",
        "# Evaluate the model on adversarial examples\n",
        "adv_test_loss, adv_test_accuracy = model.evaluate(x_test_adv, y_test)\n",
        "print(f'Test accuracy on adversarial examples: {adv_test_accuracy}')\n",
        "\n",
        "# Get the predicted labels for adversarial examples\n",
        "y_pred_prob = model.predict(x_test_adv)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')"
      ],
      "metadata": {
        "id": "sDJ1tHtF2GVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e181d929-7774-494d-b3e0-3fc0f3c67c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 10ms/step - loss: 7.9827 - accuracy: 0.0977\n",
            "Test accuracy on adversarial examples: 0.09769999980926514\n",
            "313/313 [==============================] - 3s 9ms/step\n",
            "Precision: 0.1136590611896692\n",
            "Recall: 0.10015714797275717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initial Robustness:** PGD White Box Attack"
      ],
      "metadata": {
        "id": "FQms0N8X7aSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_tf = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
        "\n",
        "# Generate adversarial examples using PGD attack in TensorFlow\n",
        "eps = 0.3\n",
        "eps_iter = 0.01\n",
        "nb_iter = 40\n",
        "eps_proj = 0.3\n",
        "adv_x = x_test_tf\n",
        "\n",
        "for i in range(nb_iter):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(adv_x)\n",
        "        predictions = model(adv_x)\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_test, predictions)\n",
        "    gradients = tape.gradient(loss, adv_x)\n",
        "    perturbation = eps_iter * tf.sign(gradients)\n",
        "    adv_x = tf.clip_by_value(adv_x + perturbation, x_test_tf - eps, x_test_tf + eps)\n",
        "    adv_x = tf.clip_by_value(adv_x, x_test_tf - eps_proj, x_test_tf + eps_proj)\n",
        "\n",
        "# Evaluate TensorFlow Keras model on adversarial test data\n",
        "adv_test_loss, adv_test_accuracy = model.evaluate(adv_x, y_test)\n",
        "print(f'Test accuracy on adversarial examples: {adv_test_accuracy}')"
      ],
      "metadata": {
        "id": "zvctVSVp4Etb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dfc1cb2-7b67-4080-a919-b959f2a0081a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 9ms/step - loss: 30.0080 - accuracy: 0.0000e+00\n",
            "Test accuracy on adversarial examples: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy on Clean Test Dataset after Retraining"
      ],
      "metadata": {
        "id": "KJJuRgV39uY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "# Extract the training data\n",
        "with ZipFile('mnist_train.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('mnist_train')\n",
        "\n",
        "# Extract the test data\n",
        "with ZipFile('mnist_test.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('mnist_test')\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = pd.read_csv('mnist_train/mnist_train.csv')\n",
        "test_data = pd.read_csv('mnist_test/mnist_test.csv')\n",
        "\n",
        "# Define x_train and y_train\n",
        "x_train = train_data.iloc[:, 1:].values\n",
        "y_train = train_data.iloc[:, 0].values\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "# Reshape input data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Randomly select 20,000 samples from the training dataset\n",
        "num_samples = 20000\n",
        "indices = np.random.choice(x_train.shape[0], num_samples, replace=False)\n",
        "x_train_subset = x_train[indices]\n",
        "y_train_subset = y_train[indices]\n",
        "\n",
        "# Load the test data\n",
        "x_test = test_data.iloc[:, 1:].values\n",
        "y_test = test_data.iloc[:, 0].values\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Reshape input data\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), strides=(1, 1), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(64, kernel_size=(3, 3), strides=(1, 1), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    BatchNormalization(),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n",
        "    BatchNormalization(),\n",
        "    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n",
        "    BatchNormalization(),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "initial_learning_rate = 0.001\n",
        "opt = Adam(lr=initial_learning_rate)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Learning rate scheduling\n",
        "def lr_schedule(epoch):\n",
        "    return initial_learning_rate * 0.9 ** epoch\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# Train the model on the subset of the training dataset\n",
        "history = model.fit(datagen.flow(x_train_subset, y_train_subset, batch_size=32),\n",
        "                    epochs=1,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[lr_scheduler])\n",
        "\n",
        "# FGSM adversarial attack function\n",
        "def fgsm_attack(model, x, y, eps=0.3):\n",
        "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "    y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(x)\n",
        "        predictions = model(x)\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)\n",
        "    gradients = tape.gradient(loss, x)\n",
        "    x_adv = tf.clip_by_value(x + eps * tf.sign(gradients), 0, 1)\n",
        "    return x_adv.numpy()\n",
        "\n",
        "# Generate adversarial examples for training data\n",
        "x_train_adv = fgsm_attack(model, x_train_subset, y_train_subset)\n",
        "\n",
        "# Concatenate original and adversarial training data\n",
        "x_combined = np.concatenate((x_train_subset, x_train_adv), axis=0)\n",
        "y_combined = np.concatenate((y_train_subset, y_train_subset), axis=0)\n",
        "\n",
        "# Shuffle the combined dataset\n",
        "combined_dataset = list(zip(x_combined, y_combined))\n",
        "np.random.shuffle(combined_dataset)\n",
        "x_combined, y_combined = zip(*combined_dataset)\n",
        "x_combined = np.array(x_combined)\n",
        "y_combined = np.array(y_combined)\n",
        "\n",
        "# Retrain the model on the combined dataset\n",
        "history = model.fit(datagen.flow(x_combined, y_combined, batch_size=32),\n",
        "                    epochs=20,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[lr_scheduler])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy after adversarial training: {test_accuracy}')\n",
        "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(f'Precision after adversarial training: {precision}')\n",
        "print(f'Recall after adversarial training: {recall}')"
      ],
      "metadata": {
        "id": "UvrjudF49Wjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "befd70fe-0dd9-4d5b-ad97-b1dfcb3405a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 34s 52ms/step - loss: 0.3952 - accuracy: 0.8921 - val_loss: 0.1009 - val_accuracy: 0.9828 - lr: 0.0010\n",
            "Epoch 1/20\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.2591 - accuracy: 0.9322 - val_loss: 0.1545 - val_accuracy: 0.9659 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.1574 - accuracy: 0.9653 - val_loss: 0.1179 - val_accuracy: 0.9778 - lr: 9.0000e-04\n",
            "Epoch 3/20\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.1338 - accuracy: 0.9736 - val_loss: 0.0986 - val_accuracy: 0.9846 - lr: 8.1000e-04\n",
            "Epoch 4/20\n",
            "1250/1250 [==============================] - 57s 45ms/step - loss: 0.1281 - accuracy: 0.9761 - val_loss: 0.1013 - val_accuracy: 0.9839 - lr: 7.2900e-04\n",
            "Epoch 5/20\n",
            "1250/1250 [==============================] - 57s 45ms/step - loss: 0.1118 - accuracy: 0.9806 - val_loss: 0.0949 - val_accuracy: 0.9860 - lr: 6.5610e-04\n",
            "Epoch 6/20\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.1042 - accuracy: 0.9834 - val_loss: 0.0841 - val_accuracy: 0.9881 - lr: 5.9049e-04\n",
            "Epoch 7/20\n",
            "1250/1250 [==============================] - 57s 46ms/step - loss: 0.0972 - accuracy: 0.9844 - val_loss: 0.0884 - val_accuracy: 0.9886 - lr: 5.3144e-04\n",
            "Epoch 8/20\n",
            "1250/1250 [==============================] - 57s 45ms/step - loss: 0.0939 - accuracy: 0.9850 - val_loss: 0.0748 - val_accuracy: 0.9912 - lr: 4.7830e-04\n",
            "Epoch 9/20\n",
            "1250/1250 [==============================] - 63s 50ms/step - loss: 0.0876 - accuracy: 0.9865 - val_loss: 0.0787 - val_accuracy: 0.9888 - lr: 4.3047e-04\n",
            "Epoch 10/20\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.0815 - accuracy: 0.9882 - val_loss: 0.0760 - val_accuracy: 0.9886 - lr: 3.8742e-04\n",
            "Epoch 11/20\n",
            "1250/1250 [==============================] - 58s 47ms/step - loss: 0.0768 - accuracy: 0.9884 - val_loss: 0.0742 - val_accuracy: 0.9895 - lr: 3.4868e-04\n",
            "Epoch 12/20\n",
            "1250/1250 [==============================] - 57s 46ms/step - loss: 0.0727 - accuracy: 0.9894 - val_loss: 0.0804 - val_accuracy: 0.9860 - lr: 3.1381e-04\n",
            "Epoch 13/20\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.0676 - accuracy: 0.9905 - val_loss: 0.0659 - val_accuracy: 0.9911 - lr: 2.8243e-04\n",
            "Epoch 14/20\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.0656 - accuracy: 0.9911 - val_loss: 0.0608 - val_accuracy: 0.9932 - lr: 2.5419e-04\n",
            "Epoch 15/20\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.0600 - accuracy: 0.9922 - val_loss: 0.0645 - val_accuracy: 0.9912 - lr: 2.2877e-04\n",
            "Epoch 16/20\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.0575 - accuracy: 0.9927 - val_loss: 0.0597 - val_accuracy: 0.9920 - lr: 2.0589e-04\n",
            "Epoch 17/20\n",
            "1250/1250 [==============================] - 66s 53ms/step - loss: 0.0551 - accuracy: 0.9930 - val_loss: 0.0583 - val_accuracy: 0.9925 - lr: 1.8530e-04\n",
            "Epoch 18/20\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.0525 - accuracy: 0.9935 - val_loss: 0.0597 - val_accuracy: 0.9916 - lr: 1.6677e-04\n",
            "Epoch 19/20\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.0518 - accuracy: 0.9933 - val_loss: 0.0622 - val_accuracy: 0.9912 - lr: 1.5009e-04\n",
            "Epoch 20/20\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.0494 - accuracy: 0.9941 - val_loss: 0.0559 - val_accuracy: 0.9924 - lr: 1.3509e-04\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0559 - accuracy: 0.9924\n",
            "Test accuracy after adversarial training: 0.9923999905586243\n",
            "313/313 [==============================] - 3s 10ms/step\n",
            "Precision after adversarial training: 0.9925874631924231\n",
            "Recall after adversarial training: 0.9923019785455216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy on Adversarial Test Dataset after Retraining"
      ],
      "metadata": {
        "id": "bToo7rgF9__v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "with ZipFile('mnist_train.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('mnist_train')\n",
        "\n",
        "with ZipFile('mnist_test.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('mnist_test')\n",
        "\n",
        "train_data = pd.read_csv('mnist_train/mnist_train.csv')\n",
        "test_data = pd.read_csv('mnist_test/mnist_test.csv')\n",
        "\n",
        "x_train = train_data.iloc[:, 1:].values\n",
        "y_train = train_data.iloc[:, 0].values\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "# Reshape input data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Randomly select 20,000 samples from the training dataset\n",
        "num_samples = 20000\n",
        "indices = np.random.choice(x_train.shape[0], num_samples, replace=False)\n",
        "x_train_subset = x_train[indices]\n",
        "y_train_subset = y_train[indices]\n",
        "\n",
        "# Generate adversarial examples for training data\n",
        "def fgsm_attack(model, x, y, eps=0.3):\n",
        "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "    y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(x)\n",
        "        predictions = model(x)\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)\n",
        "    gradients = tape.gradient(loss, x)\n",
        "    x_adv = tf.clip_by_value(x + eps * tf.sign(gradients), 0, 1)\n",
        "    return x_adv.numpy()\n",
        "\n",
        "x_train_adv = fgsm_attack(model, x_train_subset, y_train_subset)\n",
        "\n",
        "# Concatenate original and adversarial training data\n",
        "x_combined = np.concatenate((x_train_subset, x_train_adv), axis=0)\n",
        "y_combined = np.concatenate((y_train_subset, y_train_subset), axis=0)\n",
        "\n",
        "# Shuffle the combined dataset\n",
        "combined_dataset = list(zip(x_combined, y_combined))\n",
        "np.random.shuffle(combined_dataset)\n",
        "x_combined, y_combined = zip(*combined_dataset)\n",
        "x_combined = np.array(x_combined)\n",
        "y_combined = np.array(y_combined)\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")\n",
        "datagen.fit(x_combined)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), strides=(1, 1), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(64, kernel_size=(3, 3), strides=(1, 1), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    BatchNormalization(),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n",
        "    BatchNormalization(),\n",
        "    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n",
        "    BatchNormalization(),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "initial_learning_rate = 0.001\n",
        "opt = Adam(learning_rate=initial_learning_rate)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Learning rate scheduling\n",
        "def lr_schedule(epoch):\n",
        "    return initial_learning_rate * 0.9 ** epoch\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# Train the model on the combined dataset\n",
        "history = model.fit(datagen.flow(x_combined, y_combined, batch_size=32),\n",
        "                    epochs=20,\n",
        "                    validation_data=(x_test_adv, y_test),\n",
        "                    callbacks=[lr_scheduler])\n",
        "\n",
        "# Generate adversarial examples using FGSM attack for test data\n",
        "x_test_tf = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
        "eps = 0.3\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(x_test_tf)\n",
        "    predictions = model(x_test_tf)\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_test, predictions)\n",
        "gradients = tape.gradient(loss, x_test_tf)\n",
        "x_test_adv = tf.clip_by_value(x_test_tf + eps * tf.sign(gradients), 0, 1)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_accuracy}')\n",
        "\n",
        "# Evaluate the model on adversarial test data\n",
        "adv_test_loss, adv_test_accuracy = model.evaluate(x_test_adv, y_test)\n",
        "print(f'Test accuracy on adversarial examples: {adv_test_accuracy}')\n",
        "\n",
        "# Get the predicted labels for adversarial examples\n",
        "y_pred_prob = model.predict(x_test_adv)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "# Calculate precision, recall, and accuracy for adversarial examples\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vnw7MZa1da9U",
        "outputId": "85e89b79-e326-46b0-f062-e39a12c55ae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1250/1250 [==============================] - 53s 41ms/step - loss: 0.3755 - accuracy: 0.8964 - val_loss: 1.2505 - val_accuracy: 0.6461 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 0.1761 - accuracy: 0.9581 - val_loss: 1.1831 - val_accuracy: 0.6885 - lr: 9.0000e-04\n",
            "Epoch 3/20\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.1475 - accuracy: 0.9687 - val_loss: 0.4677 - val_accuracy: 0.8578 - lr: 8.1000e-04\n",
            "Epoch 4/20\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.1354 - accuracy: 0.9722 - val_loss: 0.2977 - val_accuracy: 0.9195 - lr: 7.2900e-04\n",
            "Epoch 5/20\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.1245 - accuracy: 0.9767 - val_loss: 0.2509 - val_accuracy: 0.9323 - lr: 6.5610e-04\n",
            "Epoch 6/20\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.1166 - accuracy: 0.9791 - val_loss: 0.6029 - val_accuracy: 0.8258 - lr: 5.9049e-04\n",
            "Epoch 7/20\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.1062 - accuracy: 0.9811 - val_loss: 0.5298 - val_accuracy: 0.8619 - lr: 5.3144e-04\n",
            "Epoch 8/20\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.1002 - accuracy: 0.9830 - val_loss: 0.1983 - val_accuracy: 0.9522 - lr: 4.7830e-04\n",
            "Epoch 9/20\n",
            "1250/1250 [==============================] - 49s 40ms/step - loss: 0.0923 - accuracy: 0.9856 - val_loss: 0.2541 - val_accuracy: 0.9292 - lr: 4.3047e-04\n",
            "Epoch 10/20\n",
            "1250/1250 [==============================] - 57s 45ms/step - loss: 0.0862 - accuracy: 0.9862 - val_loss: 0.2790 - val_accuracy: 0.9231 - lr: 3.8742e-04\n",
            "Epoch 11/20\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.0826 - accuracy: 0.9871 - val_loss: 0.2872 - val_accuracy: 0.9182 - lr: 3.4868e-04\n",
            "Epoch 12/20\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.0769 - accuracy: 0.9884 - val_loss: 0.1902 - val_accuracy: 0.9529 - lr: 3.1381e-04\n",
            "Epoch 13/20\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.0736 - accuracy: 0.9887 - val_loss: 0.2674 - val_accuracy: 0.9259 - lr: 2.8243e-04\n",
            "Epoch 14/20\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.0687 - accuracy: 0.9905 - val_loss: 0.1744 - val_accuracy: 0.9557 - lr: 2.5419e-04\n",
            "Epoch 15/20\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.0665 - accuracy: 0.9902 - val_loss: 0.1486 - val_accuracy: 0.9632 - lr: 2.2877e-04\n",
            "Epoch 16/20\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.0632 - accuracy: 0.9911 - val_loss: 0.1780 - val_accuracy: 0.9521 - lr: 2.0589e-04\n",
            "Epoch 17/20\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.0602 - accuracy: 0.9913 - val_loss: 0.1435 - val_accuracy: 0.9654 - lr: 1.8530e-04\n",
            "Epoch 18/20\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.0576 - accuracy: 0.9921 - val_loss: 0.1930 - val_accuracy: 0.9488 - lr: 1.6677e-04\n",
            "Epoch 19/20\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.0562 - accuracy: 0.9922 - val_loss: 0.1299 - val_accuracy: 0.9688 - lr: 1.5009e-04\n",
            "Epoch 20/20\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 0.0536 - accuracy: 0.9929 - val_loss: 0.1785 - val_accuracy: 0.9559 - lr: 1.3509e-04\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0511 - accuracy: 0.9929\n",
            "Test accuracy: 0.992900013923645\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 6.6048 - accuracy: 0.1523\n",
            "Test accuracy on adversarial examples: 0.15230000019073486\n",
            "313/313 [==============================] - 4s 11ms/step\n",
            "Precision: 0.20463383831109366\n",
            "Recall: 0.15626776281692786\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwzGq89bjyTdy91shguveo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}